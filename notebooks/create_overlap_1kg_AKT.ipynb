{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Cannot run multiple SparkContexts at once; existing SparkContext(app=pyspark-shell, master=spark://192.168.252.42:7077) created by __init__ at <ipython-input-1-516bb5d3dfb8>:23 ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-516bb5d3dfb8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0mplot_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"/home/ubuntu/data/tmp\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m \u001b[0msc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSparkContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0mhadoop_config\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhadoopConfiguration\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0mhadoop_config\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"fs.s3a.access.key\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"8YY584J59H7Q6AVKHSU8\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venv/lib/python3.6/site-packages/pyspark/context.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls)\u001b[0m\n\u001b[1;32m    131\u001b[0m                     \" note this option will be removed in Spark 3.0\")\n\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 133\u001b[0;31m         \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgateway\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgateway\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    134\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m             self._do_init(master, appName, sparkHome, pyFiles, environment, batchSize, serializer,\n",
      "\u001b[0;32m~/venv/lib/python3.6/site-packages/pyspark/context.py\u001b[0m in \u001b[0;36m_ensure_initialized\u001b[0;34m(cls, instance, gateway, conf)\u001b[0m\n\u001b[1;32m    330\u001b[0m                         \u001b[0;34m\" created by %s at %s:%s \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    331\u001b[0m                         % (currentAppName, currentMaster,\n\u001b[0;32m--> 332\u001b[0;31m                             callsite.function, callsite.file, callsite.linenum))\n\u001b[0m\u001b[1;32m    333\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    334\u001b[0m                     \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_active_spark_context\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minstance\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Cannot run multiple SparkContexts at once; existing SparkContext(app=pyspark-shell, master=spark://192.168.252.42:7077) created by __init__ at <ipython-input-1-516bb5d3dfb8>:23 "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import hail as hl\n",
    "import pyspark\n",
    "import bokeh\n",
    "import logging\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "import pickle \n",
    "from matplotlib import pyplot as plt\n",
    "from typing import Any, Counter, List, Optional, Tuple, Union,Dict,Set\n",
    "from hail.plot import show, output_notebook\n",
    "from bokeh.palettes import d3  # pylint: disable=no-name-in-module\n",
    "from bokeh.models import Plot, Row, Span, NumeralTickFormatter, LabelSet\n",
    "from gnomad.utils.plotting import *\n",
    "from typing import Set, Tuple\n",
    "\n",
    "tmp_dir = \"hdfs://spark-master:9820/\"\n",
    "temp_dir = \"file:///home/ubuntu/data/tmp\"\n",
    "plot_dir = \"/home/ubuntu/data/tmp\"\n",
    "\n",
    "sc = pyspark.SparkContext()\n",
    "hadoop_config = sc._jsc.hadoopConfiguration()\n",
    "hadoop_config.set(\"fs.s3a.access.key\", \"8YY584J59H7Q6AVKHSU8\")\n",
    "hadoop_config.set(\"fs.s3a.secret.key\", \"P8vePa7JUvxKXX2me9ti1cGujgYWMoimAwx4mMlM\")\n",
    "hadoop_config.set(\"fs.gs.impl\", \"com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystem\")\n",
    "hadoop_config.set(\"fs.AbstractFileSystem.gs.impl\", \"com.google.cloud.hadoop.fs.gcs.GoogleHadoopFS\")\n",
    "hl.init(sc=sc, tmp_dir=tmp_dir, default_reference='GRCh38')\n",
    "output_notebook()\n",
    "logging.basicConfig(format=\"%(levelname)s (%(name)s %(lineno)s): %(message)s\")\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "mt = hl.import_vcf(f\"{temp_dir}/ddd-elgh-ukbb/WES_AKT_1kg_intersection.vcf.gz\",reference_genome='GRCh38', force_bgz=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-01-19 15:38:49 Hail: INFO: Coerced sorted dataset\n",
      "2021-01-19 15:39:28 Hail: INFO: wrote matrix table with 29986 rows and 2504 columns in 3 partitions to hdfs://spark-master:9820//WES_AKT_1kg_intersection.mt\n",
      "    Total size: 433.24 MiB\n",
      "    * Rows/entries: 433.23 MiB\n",
      "    * Columns: 9.85 KiB\n",
      "    * Globals: 11.00 B\n",
      "    * Smallest partition: 9893 rows (144.30 MiB)\n",
      "    * Largest partition:  10063 rows (144.47 MiB)\n"
     ]
    }
   ],
   "source": [
    "mt.write(f\"{tmp_dir}/WES_AKT_1kg_intersection.mt\", overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
